{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3554f03b",
      "metadata": {
        "id": "3554f03b"
      },
      "source": [
        "# Neural Network base\n",
        "In this lab, you will learn the basement of a neural network. You will build up several base modules of a neural network and finally your own deep neural network!\n",
        "\n",
        "**Outline**\n",
        "- [1 - Initialization parameters](#1)\n",
        "- [2 - Forward Propagation Module](#2)\n",
        "    - [2.1 - Linear Forward](#2-1)\n",
        "    - [2.2 - Activation Function](#2-2)\n",
        "    - [2.3 - L-layer linear_activation_forward](#2-3)\n",
        "- [3 - Cost Function](#3)\n",
        "- [4 - Backward Propagation Module](#4)\n",
        "    - [4.1 - Linear Backward](#4-1)\n",
        "    - [4.2 - Linear-Activation Backward](#4-2)\n",
        "    - [4.3 - L-layer Backward](#4-3)\n",
        "    - [4.4 - Update Parameters](#4-4)\n",
        "- [5 - Build up your own network](#5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33fdf66e",
      "metadata": {
        "id": "33fdf66e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8cbc8b0",
      "metadata": {
        "id": "c8cbc8b0"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Initialization parameters\n",
        "Weight initialization is an important design choice when developing deep learning neural network models.\n",
        "\n",
        "Neural network models are fit using an optimization algorithm called stochastic gradient descent that incrementally changes the network weights to minimize a loss function, hopefully resulting in a set of weights for the mode that is capable of making useful predictions. This optimization algorithm requires a starting point in the space of possible weight values from which to begin the optimization process. Weight initialization is a procedure to set the weights of a neural network to small random values that define the starting point for the optimization (learning or training) of the neural network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a682ca7",
      "metadata": {
        "id": "6a682ca7"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1) #please check the useage of random seed and try different seeds\n",
        "\n",
        "    W1 = np.random.randn(n_h,n_x)*0.01\n",
        "    b1 = np.zeros((n_h,1)) # here we set bias value as zero, please try different values\n",
        "    W2 = np.random.randn(n_y,n_h)*0.01\n",
        "    b2 = np.zeros((n_y,1))\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ea63c6",
      "metadata": {
        "id": "25ea63c6"
      },
      "outputs": [],
      "source": [
        "parameters = initialize_parameters(3,2,1)\n",
        "\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1fd0d63",
      "metadata": {
        "id": "d1fd0d63"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Forward Propagation Module\n",
        "Forward propagation is where input data is fed through a network, in a forward direction, to generate an output. The data is accepted by hidden layers and processed, as per the activation function, and moves to the successive layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ad0502",
      "metadata": {
        "id": "03ad0502"
      },
      "source": [
        "<a name='2-1'></a>\n",
        "### 2.1 - Linear Forward Propagation\n",
        "The linear forward module (vectorized over all the examples) computes the following equations:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "where $A^{[0]} = X$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d242c3",
      "metadata": {
        "id": "e4d242c3"
      },
      "outputs": [],
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter\n",
        "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    Z = np.dot(W,A) + b\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ce8d37",
      "metadata": {
        "id": "21ce8d37"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "t_A = np.random.randn(5,1)\n",
        "t_W = np.random.randn(5,5)\n",
        "t_b = np.random.randn(5,1)\n",
        "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
        "print(\"Z = \" + str(t_Z))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed29447a",
      "metadata": {
        "id": "ed29447a"
      },
      "source": [
        "<a name='2-2'></a>\n",
        "### 2.2 - Activation Function\n",
        "The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
        "\n",
        "In this notebook, you will use two activation functions:\n",
        "\n",
        "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
        "\n",
        "\n",
        "- **ReLU**: The mathematical formula for ReLu is $A = RELU(Z) = max(0, Z)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2728c5",
      "metadata": {
        "id": "5c2728c5"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    Z_activation = 1 / (1 + np.exp(-Z))\n",
        "    cache = (Z)\n",
        "    return Z_activation , cache\n",
        "\n",
        "def relu(Z):\n",
        "    Z_activation = np.maximum(Z , 0)\n",
        "    cache = (Z)\n",
        "    return Z_activation , cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db6497d4",
      "metadata": {
        "id": "db6497d4"
      },
      "outputs": [],
      "source": [
        "def activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value\n",
        "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe77af3c",
      "metadata": {
        "id": "fe77af3c"
      },
      "outputs": [],
      "source": [
        "t_A = np.random.randn(5,1)\n",
        "t_W = np.random.randn(5,5)\n",
        "t_b = np.random.randn(5,1)\n",
        "\n",
        "t_Z, t_linear_cache = activation_forward(t_A , t_W , t_b, activation = \"sigmoid\")\n",
        "print(\"With sigmoid: Z = \" + str(t_Z))\n",
        "\n",
        "t_Z, t_linear_cache = activation_forward(t_A , t_W , t_b, activation = \"relu\")\n",
        "print(\"With relu: Z = \" + str(t_Z))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b587543a",
      "metadata": {
        "id": "b587543a"
      },
      "source": [
        "<a name='2-3'></a>\n",
        "### 2.3 - L-layer linear_activation_forward\n",
        "For even more convenience when implementing the  ð¿ -layer Neural Net, you will need a function that replicates the previous one (linear_activation_forward with **RELU**)  **ð¿âˆ’1**  times, then follows that with **one** linear_activation_forward with **SIGMOID**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "560f46d1",
      "metadata": {
        "id": "560f46d1"
      },
      "outputs": [],
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "\n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "\n",
        "    Returns:\n",
        "    AL -- activation value from the output (last) layer\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2   # number of layers in the neural network\n",
        "\n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    # The for loop starts at 1 because layer 0 is the input\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    AL, cache = activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL, caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f2f471",
      "metadata": {
        "id": "85f2f471"
      },
      "outputs": [],
      "source": [
        "t_X = np.random.randn(3,1)\n",
        "t_parameters = initialize_parameters(3,2,3)\n",
        "t_AL, t_caches = L_model_forward(t_X, t_parameters)\n",
        "print(t_AL.shape)\n",
        "print(\"AL = \" + str(t_AL))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39454e1d",
      "metadata": {
        "id": "39454e1d"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Cost function\n",
        "A cost function is a measure of error between what value your model predicts and what the value actually is.\n",
        "\n",
        "Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ced01d",
      "metadata": {
        "id": "e9ced01d"
      },
      "outputs": [],
      "source": [
        "def compute_cost(AL, Y):\n",
        "\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- MSE cost\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[0]\n",
        "\n",
        "    cost = (-np.dot(Y,np.log(AL.T))-np.dot((1-Y),np.log(1-AL.T)))/m\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c09a9b",
      "metadata": {
        "id": "12c09a9b"
      },
      "outputs": [],
      "source": [
        "t_Y = np.array([1 , 0 , 1])\n",
        "t_AL = np.array([0.9 , 0.18 , 0.89])\n",
        "t_cost = compute_cost(t_AL, t_Y)\n",
        "\n",
        "print(\"Cost: \" + str(t_cost))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e409899",
      "metadata": {
        "id": "9e409899"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4 - Backward Propagation Module\n",
        "Backpropagation is a process involved in training a neural network. It involves taking the error rate of a forward propagation and feeding this loss backward through the neural network layers to fine-tune the weights.\n",
        "\n",
        "Just as you did for the forward propagation, you'll implement functions for backpropagation. Remember that backpropagation is used to calculate the gradient of the loss function with respect to the parameters.\n",
        "\n",
        "Similarly to forward propagation, you're going to build the backward propagation in three steps:\n",
        "1. LINEAR backward\n",
        "2. LINEAR -> **ACTIVATION** backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
        "3. [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1491abf1",
      "metadata": {
        "id": "1491abf1"
      },
      "source": [
        "<a name='4-1'></a>\n",
        "### 4.1 -  Linear Backward\n",
        "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
        "\n",
        "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
        "\n",
        "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\n",
        "\n",
        "Here are the formulas you need:\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n",
        "\n",
        "\n",
        "$A^{[l-1] T}$ is the transpose of $A^{[l-1]}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba6f930",
      "metadata": {
        "id": "8ba6f930"
      },
      "outputs": [],
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "\n",
        "    dW = np.dot(dZ, A_prev.T)/m\n",
        "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62258170",
      "metadata": {
        "id": "62258170"
      },
      "outputs": [],
      "source": [
        "t_dZ = np.random.randn(5,1)\n",
        "t_linear_cache = (np.random.randn(5,1),np.random.randn(5,5),np.random.randn(5,1))\n",
        "# t_linear_cache = (A_prev, W, b)\n",
        "t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n",
        "\n",
        "print(\"dA_prev: \" + str(t_dA_prev))\n",
        "print(\"dW: \" + str(t_dW))\n",
        "print(\"db: \" + str(t_db))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8b292d",
      "metadata": {
        "id": "db8b292d"
      },
      "source": [
        "<a name='4-2'></a>\n",
        "### 4.2 -  Linear-Activation Backward\n",
        "Next, you will create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**.\n",
        "\n",
        "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n",
        "\n",
        "```python\n",
        "dZ = sigmoid_backward(dA, activation_cache)\n",
        "```\n",
        "\n",
        "- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n",
        "\n",
        "```python\n",
        "dZ = relu_backward(dA, activation_cache)\n",
        "```\n",
        "\n",
        "If $g(.)$ is the activation function,\n",
        "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}). \\tag{11}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4348e409",
      "metadata": {
        "id": "4348e409"
      },
      "outputs": [],
      "source": [
        "def sigmoid_backward(dA, Z): # Z is the activation_cache\n",
        "    sig , catch = sigmoid(Z)\n",
        "    return dA * sig * (1 - sig)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0;\n",
        "    return dZ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d1940e9",
      "metadata": {
        "id": "0d1940e9"
      },
      "outputs": [],
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l\n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43fa887d",
      "metadata": {
        "id": "43fa887d"
      },
      "outputs": [],
      "source": [
        "t_dAL = np.random.randn(5,1)\n",
        "t_linear_cache = (np.random.randn(5,1),np.random.randn(5,5),np.random.randn(5,1))\n",
        "t_activation_cache =(np.random.randn(5,1))\n",
        "\n",
        "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, (t_linear_cache,t_activation_cache), activation = \"sigmoid\")\n",
        "print(\"With sigmoid: dA_prev = \" + str(t_dA_prev))\n",
        "print(\"With sigmoid: dW = \" + str(t_dW))\n",
        "print(\"With sigmoid: db = \" + str(t_db))\n",
        "\n",
        "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, (t_linear_cache,t_activation_cache), activation = \"relu\")\n",
        "print(\"With relu: dA_prev = \" + str(t_dA_prev))\n",
        "print(\"With relu: dW = \" + str(t_dW))\n",
        "print(\"With relu: db = \" + str(t_db))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e603bd",
      "metadata": {
        "id": "29e603bd"
      },
      "source": [
        "<a name='4-3'></a>\n",
        "### 4.3 -  L-layer Backward\n",
        "Recall that when you implemented the `L_model_forward` function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you'll use those variables to compute the gradients. Therefore, in the `L_model_backward` function, you'll iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$.\n",
        "\n",
        "**Initializing backpropagation**:\n",
        "\n",
        "To backpropagate through this network, you know that the output is:\n",
        "$A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
        "To do so, use this formula (derived using calculus which, again, you don't need in-depth knowledge of:\n",
        "```python\n",
        "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
        "```\n",
        "\n",
        "After that, you will have to use a `for` loop to iterate through all the other layers using the LINEAR->RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :\n",
        "\n",
        "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
        "\n",
        "For example, for $l=3$ this would store $dW^{[l]}$ in `grads[\"dW3\"]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f6d9b5",
      "metadata": {
        "id": "89f6d9b5"
      },
      "outputs": [],
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "\n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ...\n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "\n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp\n",
        "    grads[\"db\" + str(L)] = db_temp\n",
        "\n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l+1)] = dW_temp\n",
        "        grads[\"db\" + str(l+1)] = db_temp\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb90840",
      "metadata": {
        "id": "9fb90840"
      },
      "outputs": [],
      "source": [
        "t_X = np.random.randn(3,1)\n",
        "t_Y_assess = np.random.randn(3,1)\n",
        "t_parameters = initialize_parameters(3,2,3)\n",
        "t_AL, t_caches = L_model_forward(t_X, t_parameters)\n",
        "\n",
        "grads = L_model_backward(t_AL, t_Y_assess, t_caches)\n",
        "\n",
        "print(\"dA0 = \" + str(grads['dA0']))\n",
        "print(\"dA1 = \" + str(grads['dA1']))\n",
        "print(\"dW1 = \" + str(grads['dW1']))\n",
        "print(\"dW2 = \" + str(grads['dW2']))\n",
        "print(\"db1 = \" + str(grads['db1']))\n",
        "print(\"db2 = \" + str(grads['db2']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d83db1b",
      "metadata": {
        "id": "3d83db1b"
      },
      "source": [
        "<a name='4-4'></a>\n",
        "### 4.4 -  Update_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b996361",
      "metadata": {
        "id": "5b996361"
      },
      "outputs": [],
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "\n",
        "    Arguments:\n",
        "    params -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "                  parameters[\"W\" + str(l)] = ...\n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    parameters = params.copy()\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "\n",
        "    for l in range(L):\n",
        "        # parameters[\"W\" + str(l+1)] = ...\n",
        "        # parameters[\"b\" + str(l+1)] = ...\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e09c4b",
      "metadata": {
        "id": "e9e09c4b"
      },
      "outputs": [],
      "source": [
        "t_X = np.random.randn(3,1)\n",
        "t_Y_assess = np.random.randn(3,1)\n",
        "t_AL, t_caches = L_model_forward(t_X, t_parameters)\n",
        "\n",
        "t_parameters = initialize_parameters(3,2,3)\n",
        "grads = L_model_backward(t_AL, t_Y_assess, t_caches)\n",
        "\n",
        "t_parameters = update_parameters(t_parameters, grads, 0.1)\n",
        "print (\"W1 = \"+ str(t_parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(t_parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(t_parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(t_parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b679d6",
      "metadata": {
        "id": "83b679d6"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5 - Build up your own network\n",
        "\n",
        "### Now you have implemented all the functions required for building a deep neural network, it's time to use these blocks to build up your own neural network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aea6d11",
      "metadata": {
        "id": "3aea6d11"
      },
      "outputs": [],
      "source": [
        "# putting these things together\n",
        "def train(X, Y, nn_hiden, epochs, learning_rate):\n",
        "    params_values = initialize_parameters(X.shape[0] , nn_hiden, 1)\n",
        "    cost_history = []\n",
        "    accuracy_history = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        Y_hat, cashe = L_model_forward(X, params_values)\n",
        "        cost = compute_cost(Y_hat.flatten(), Y)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        ###one simplified example to display the accuracy score of classification\n",
        "        Y_hat_bin = np.where(Y_hat>0.5,1,0)\n",
        "        accuracy = precision_score(Y , Y_hat_bin.flatten(),average='micro')\n",
        "        accuracy_history.append(accuracy)\n",
        "\n",
        "        grads_values = L_model_backward(Y_hat, Y, cashe)\n",
        "        params_values = update_parameters(params_values, grads_values, learning_rate)\n",
        "        if (i+1) %100==0:\n",
        "            print(f\"Epoch #{i+1}: train loss: {cost}; precision score: {accuracy}\")\n",
        "\n",
        "    return params_values, cost_history, accuracy_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b66d630",
      "metadata": {
        "id": "0b66d630"
      },
      "source": [
        "## Apply our neural network on a sample data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f99c8c",
      "metadata": {
        "id": "b1f99c8c"
      },
      "source": [
        "### Data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d02f87",
      "metadata": {
        "id": "d5d02f87"
      },
      "outputs": [],
      "source": [
        "X = np.array([[21.04,5,0.5,90], [14.16,3,1,80], [8.52,2,0.5,70], [7.52,2.3,1,80]])\n",
        "y = np.array([0, 0, 1 , 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874af863",
      "metadata": {
        "scrolled": false,
        "id": "874af863"
      },
      "outputs": [],
      "source": [
        "params_values, cost_history, accuracy_history = train(X, y, 5, 5000 , 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4119923",
      "metadata": {
        "id": "c4119923"
      },
      "source": [
        "### You can calculate the training accuracy by using the methods below:\n",
        "[Classfication metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ec1b68",
      "metadata": {
        "id": "08ec1b68"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}